{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯\n",
    "## 概述\n",
    "贝叶斯分类算法是统计学中一种 **概率分类** 方法，朴素贝叶斯分类是贝叶斯分类中最简单的一种。分类原理就是利用贝叶斯公司根据某特征的 **先验概率** 计算出其 **后验概率**，然后选择具有较大后验概率的类作为该特征所属的类。之所以称为“朴素”，是因为贝叶斯分类只做最原始、最简单的假设：所有特征之间是 **统计独立** 的。\n",
    "\n",
    "假设某样本 $X$ 有 $a_1,a_2,\\cdots ,a_n$ 个属性，那么有\n",
    "$$\n",
    "P(X) = P(a_1,a_2,\\cdots ,a_n) = \\prod_{i=1}^n P(a_i|X)\n",
    "$$\n",
    "满足这样的公式就说明特征统计独立\n",
    "\n",
    "### 1、条件概率公式\n",
    "条件概率 $(Condition Probability)$，就是指在事件 $B$ 发生的情况下，$A$ 发生的概率，用 $P(A|B)$ 表示。\n",
    "![文氏图](\\images/文氏图.png)\n",
    "\n",
    "根据文氏图可知，在事件 $B$ 发生的情况下，事件 $A$ 发生的概率就是 $\\displaystyle \\frac{P(A \\cap B)}{P(B)}$\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\implies P(A \\cap B) = p(B)p(A|B)\n",
    "$$\n",
    "同理可得\n",
    "$$\n",
    "P(A \\cap B) = P(A)P(B|A)\n",
    "$$\n",
    "所以\n",
    "$$\n",
    "P(A|B)P(B) = P(B|A)P(A) \\implies P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "全概率公式，如果事件 $A_1,A_2,\\cdots ,A_n$ 构成一个完备事件且都有正概率，那么对于任意一个事件 $B$ 有：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    P(B)&= P(BA_1)+P(BA_2)+\\cdots +P(BA_n) \\\\\n",
    "    &= P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\\cdots +P(B|A_n)P(A_n)\n",
    "\\end{aligned}\n",
    "P(B) = \\sum_{i=1}^{n}P(B|A_i)P(A_i)\n",
    "$$\n",
    "\n",
    "### 2、贝叶斯推断\n",
    "根据条件概率和全概率公式，可以得到贝叶斯公式如下：\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\\\\n",
    "P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{\\sum_{i=1}^{n}P(B|A_i)P(A_i)}\n",
    "$$\n",
    "其中\n",
    "\n",
    "$P(A)$ 称为 **先验概率（Prior Probability）**，即在事件 $B$ 发生之前，对事件 $A$ 概率的一个判断\n",
    "\n",
    "$P(A|B)$ 称为 **后验概率（Posterior Probability）**，即在事件 $B$ 发生之后，对事件 $A$ 概率的一个重新评估\n",
    "\n",
    "$\\displaystyle \\frac{P(B|A)}{P(B)}$ 称为 **可能性函数**，这是一个调整因子，是的预估概率更接近实际概率。\n",
    "\n",
    "所以条件概率可以理解为：**后验概率 = 先验概率 * 调整因子**\n",
    "\n",
    "- 若可能性函数 > 1，意味着先验概率被增强，事件 $A$ 发生的可能性变大\n",
    "- 若可能性函数 = 1，意味着事件 $B$ 无助于判断事件 $A$ 的可能性\n",
    "- 若可能性函数 < 1，意味着先验概率被削弱，事件 $A$ 发生的可能性变小\n",
    "\n",
    "## 朴素贝叶斯种类\n",
    "在 `scikit-learn` 中，有三种朴素贝叶斯的分类算法。分别是 `GaussianNB`、`MultinomialNB` 和 `BernoulliNB`。\n",
    "\n",
    "### 1、GaussianNB\n",
    "`GaussianNB` 是先验为 **高斯分布(正态分布)** 的朴素贝叶斯，假设每个标签的数据都服从简单的正态分布。\n",
    "$$\n",
    "P(X_j = x_i|Y = C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\\right)\n",
    "$$\n",
    "其中 $C_k$ 为 $Y$ 的第 $k$ 类类别。$\\mu_k、\\sigma^{2}_k$ 需要从训练集估计\n",
    "\n",
    "使用 `scilit-learn` 简单实现 `GaussianNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 导入数据集\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "# 切分数据集\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data,\n",
    "                                                iris.target,\n",
    "                                                random_state=12)\n",
    "\n",
    "# 建模\n",
    "clf = GaussianNB()\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000, 2.32926069e-017, 1.81656357e-023],\n",
       "       [4.28952299e-154, 2.48576754e-002, 9.75142325e-001],\n",
       "       [1.00000000e+000, 7.45528845e-018, 3.79800436e-024],\n",
       "       [3.59748710e-076, 9.99751806e-001, 2.48194200e-004],\n",
       "       [2.20411871e-239, 4.45798016e-009, 9.99999996e-001],\n",
       "       [1.23795145e-173, 1.95814902e-003, 9.98041851e-001],\n",
       "       [2.45866589e-206, 2.34481513e-007, 9.99999766e-001],\n",
       "       [1.00000000e+000, 2.61810906e-017, 2.67446831e-023],\n",
       "       [3.07448595e-259, 9.07196639e-011, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.14549667e-010, 3.00314173e-017],\n",
       "       [1.64566141e-101, 9.87428016e-001, 1.25719837e-002],\n",
       "       [1.00000000e+000, 5.62770009e-016, 8.77233124e-022],\n",
       "       [1.00000000e+000, 9.78098062e-014, 4.81247272e-020],\n",
       "       [1.00000000e+000, 3.96616431e-015, 3.17162008e-021],\n",
       "       [2.58159395e-110, 7.85918892e-001, 2.14081108e-001],\n",
       "       [8.01004975e-208, 8.36611920e-006, 9.99991634e-001],\n",
       "       [2.27845999e-193, 5.52863568e-004, 9.99447136e-001],\n",
       "       [2.52133012e-090, 9.94597495e-001, 5.40250471e-003],\n",
       "       [1.00000000e+000, 4.06675976e-017, 2.53312064e-023],\n",
       "       [3.29537129e-123, 9.22312452e-001, 7.76875484e-002],\n",
       "       [1.00000000e+000, 4.66765440e-017, 1.99662820e-023],\n",
       "       [7.54708431e-074, 9.99690656e-001, 3.09343577e-004],\n",
       "       [6.27117035e-136, 1.83265786e-001, 8.16734214e-001],\n",
       "       [4.68960290e-103, 9.82756006e-001, 1.72439943e-002],\n",
       "       [1.00000000e+000, 2.15636250e-014, 2.25086772e-020],\n",
       "       [5.92924136e-199, 5.41122729e-007, 9.99999459e-001],\n",
       "       [4.07679795e-141, 7.38689632e-002, 9.26131037e-001],\n",
       "       [2.77929930e-083, 9.99806458e-001, 1.93541791e-004],\n",
       "       [1.00000000e+000, 4.48465501e-017, 4.36464333e-023],\n",
       "       [1.00000000e+000, 1.64440161e-014, 1.13341951e-021],\n",
       "       [1.00000000e+000, 8.68192867e-017, 6.71630735e-023],\n",
       "       [7.15007036e-050, 9.99997055e-001, 2.94492877e-006],\n",
       "       [1.73414331e-178, 2.06441448e-003, 9.97935586e-001],\n",
       "       [1.00000000e+000, 4.90168069e-019, 3.86471595e-024],\n",
       "       [1.35600871e-156, 2.28929843e-002, 9.77107016e-001],\n",
       "       [1.00000000e+000, 1.78544881e-015, 1.09390819e-020],\n",
       "       [1.86074590e-058, 9.99948860e-001, 5.11400371e-005],\n",
       "       [3.69548269e-057, 9.99992986e-001, 7.01435008e-006]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在测试集上执行预测，proba导出的是每个样本属于某类的概率\n",
    "clf.predict(Xtest)\n",
    "clf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试准确率\n",
    "accuracy_score(ytest, clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、MultinomialNB\n",
    "`MultinomialNB` 是先验为 **多项式分布** 的朴素贝叶斯，假设每个标签的数据都服从多项分布。它假设特征是由一个简单多项式分布生成的。\n",
    "多项分布可以描述各种类型样本出现次数的概率，因此多项式朴素贝叶斯非常适合用于描述出现次数或\n",
    "者出现次数比例的特征。该模型常用于文本分类，特征表示的是次数，例如某个词语的出现次数。\n",
    "多项式分布公式如下：\n",
    "$$\n",
    "P(X_j = x_{jl}|Y = C_k) = \\frac{x_{jl}+\\lambda}{m_k+n\\lambda}\n",
    "$$\n",
    "\n",
    "其中,$P(X_j = x_{jl}|Y = C_k)$ 是第 $k$ 个类别的第 $j$ 维特征的第 $l$ 个取值条件的概率\n",
    "\n",
    "$m_k$ 是训练集中输出为第 $k$ 类的样本个数.\n",
    "\n",
    "$\\lambda$ 为一个大于0的常数，常常取为1，即拉普拉斯平滑。也可以取其他值。\n",
    "\n",
    "### 3、BernoulliNB\n",
    "`BernoulliNB` 是先验为 **伯努利分布** 的朴素贝叶斯，假设特征的先验概率为二元伯努利分布，即如下式：\n",
    "$$\n",
    "P(X_j = x_{jl}|Y = C_k) = P(j|Y = C_k)x_{jl} + (1-P(j|Y = C_k))(1-x_{jl})\n",
    "$$\n",
    "此时 $l$ 只有两种取值,$x_{jl}$ 只能取1或0.\n",
    "\n",
    "在伯努利模型中，每个特征的取值是布尔型的，即true和false，或者1和0。在文本分类中，就是一个特征有没有在一个文档中出现。\n",
    "\n",
    "### 总结\n",
    "- 一般来说，如果样本特征的分布大部分是连续值，使用 `GaussianNB` 会比较好。\n",
    "- 如果样本特征的分布大部分是多元离散值，使用 `MultinomialNB` 比较合适。\n",
    "- 而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用 `BernoulliNB`。\n",
    "\n",
    "## 鸢尾花数据实验\n",
    "使用 `GaussianNB` 对鸢尾花数据进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3            4\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 导入数据集\n",
    "dataSet = pd.read_csv('../03-Navie_Byes/expDataSet/iris.txt', header=None)\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分训练集和测试集\n",
    "\n",
    "def randSplit(dataSet, rate):\n",
    "    \"\"\"随机切分数据集和训练集\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataSet : \n",
    "        原始数据集\n",
    "    rate : \n",
    "        训练集占比\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train, test :\n",
    "        随机切分的训练集和测试集合\n",
    "    \"\"\"    \n",
    "    l = list(dataSet.index)                 #提取出索引\n",
    "    random.shuffle(l)                       #随机打乱索引\n",
    "    dataSet.index = l                       #将打乱后的索引重新赋值给原数据集\n",
    "    n = dataSet.shape[0]                    #总行数\n",
    "    m = int(n * rate)                       #训练集的数量\n",
    "    train = dataSet.loc[range(m), :]        #提取前m个记录作为训练集\n",
    "    test = dataSet.loc[range(m, n), :]      #剩下的作为测试集\n",
    "    dataSet.index = range(dataSet.shape[0]) #更新原数据集的索引\n",
    "    test.index = range(test.shape[0])       #更新测试集的索引\n",
    "    return train, test\n",
    "\n",
    "train, test = randSplit(dataSet, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建朴素贝叶斯分类器\n",
    "def gnb_classify(train,test):\n",
    "    labels = train.iloc[:,-1].value_counts().index #提取训练集的标签种类\n",
    "    mean =[] #存放每个类别的均值\n",
    "    std =[] #存放每个类别的方差\n",
    "    result = [] #存放测试集的预测结果\n",
    "    for i in labels:\n",
    "        item = train.loc[train.iloc[:,-1]==i,:] #分别提取出每一种类别\n",
    "        m = item.iloc[:,:-1].mean() #当前类别的平均值\n",
    "        s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #当前类别的方差\n",
    "        mean.append(m) #将当前类别的平均值追加至列表\n",
    "        std.append(s) #将当前类别的方差追加至列表\n",
    "    means = pd.DataFrame(mean,index=labels) #变成DF格式，索引为类标签\n",
    "    stds = pd.DataFrame(std,index=labels) #变成DF格式，索引为类标签\n",
    "    for j in range(test.shape[0]):\n",
    "        iset = test.iloc[j,:-1].tolist() #当前测试实例\n",
    "        iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) #正态分布公式\n",
    "        prob = 1 #初始化当前实例总概率\n",
    "        for k in range(test.shape[1]-1): #遍历每个特征\n",
    "            prob *= iprob[k] #特征概率之积即为当前实例概率\n",
    "            cla = prob.index[np.argmax(prob.values)] #返回最大概率的类别\n",
    "        result.append(cla)\n",
    "    test['predict'] = result #添加预测结果列\n",
    "    acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #计算预测准确率\n",
    "    print(f'模型预测准确率为{acc}')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型预测准确率为1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3                4          predict\n",
       "0   5.8  2.7  5.1  1.9   Iris-virginica   Iris-virginica\n",
       "1   6.3  2.5  5.0  1.9   Iris-virginica   Iris-virginica\n",
       "2   5.0  3.6  1.4  0.2      Iris-setosa      Iris-setosa\n",
       "3   6.5  3.0  5.2  2.0   Iris-virginica   Iris-virginica\n",
       "4   6.8  2.8  4.8  1.4  Iris-versicolor  Iris-versicolor\n",
       "5   6.4  3.2  4.5  1.5  Iris-versicolor  Iris-versicolor\n",
       "6   4.8  3.1  1.6  0.2      Iris-setosa      Iris-setosa\n",
       "7   5.0  3.4  1.5  0.2      Iris-setosa      Iris-setosa\n",
       "8   5.6  2.7  4.2  1.3  Iris-versicolor  Iris-versicolor\n",
       "9   6.4  2.8  5.6  2.1   Iris-virginica   Iris-virginica\n",
       "10  5.1  2.5  3.0  1.1  Iris-versicolor  Iris-versicolor\n",
       "11  6.9  3.1  5.4  2.1   Iris-virginica   Iris-virginica\n",
       "12  5.5  2.6  4.4  1.2  Iris-versicolor  Iris-versicolor\n",
       "13  5.9  3.0  4.2  1.5  Iris-versicolor  Iris-versicolor\n",
       "14  5.6  2.9  3.6  1.3  Iris-versicolor  Iris-versicolor\n",
       "15  5.8  2.8  5.1  2.4   Iris-virginica   Iris-virginica\n",
       "16  6.0  3.4  4.5  1.6  Iris-versicolor  Iris-versicolor\n",
       "17  7.1  3.0  5.9  2.1   Iris-virginica   Iris-virginica\n",
       "18  6.5  3.0  5.8  2.2   Iris-virginica   Iris-virginica\n",
       "19  6.3  2.3  4.4  1.3  Iris-versicolor  Iris-versicolor\n",
       "20  6.7  3.1  4.7  1.5  Iris-versicolor  Iris-versicolor\n",
       "21  5.4  3.4  1.7  0.2      Iris-setosa      Iris-setosa\n",
       "22  6.2  3.4  5.4  2.3   Iris-virginica   Iris-virginica\n",
       "23  6.4  2.8  5.6  2.2   Iris-virginica   Iris-virginica\n",
       "24  4.9  3.1  1.5  0.1      Iris-setosa      Iris-setosa\n",
       "25  5.4  3.9  1.7  0.4      Iris-setosa      Iris-setosa\n",
       "26  5.4  3.7  1.5  0.2      Iris-setosa      Iris-setosa\n",
       "27  5.5  4.2  1.4  0.2      Iris-setosa      Iris-setosa\n",
       "28  5.2  3.5  1.5  0.2      Iris-setosa      Iris-setosa\n",
       "29  5.6  3.0  4.1  1.3  Iris-versicolor  Iris-versicolor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_classify(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.8666666666666667\n",
      "模型预测准确率为1.0\n",
      "模型预测准确率为0.9\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为1.0\n",
      "模型预测准确率为0.8666666666666667\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train,test= randSplit(dataSet, 0.8)\n",
    "    gnb_classify(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档分类实验\n",
    "朴素贝叶斯一个很重要的应用就是文本分类，所以我们以在线社区留言为例。为了不影响社区的发\n",
    "展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语\n",
    "言，那么就将该留言标志为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型：侮\n",
    "辱类和非侮辱类，使用1和0分别表示。\n",
    "\n",
    "我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，\n",
    "再决定将哪些单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向\n",
    "量。简单起见，我们先假设已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、构建词向量\n",
    "说明：留言文本已经被切分好，并且认为标注好类别，用于训练模型。\n",
    "\n",
    "类别有两类，分别为\n",
    "- 侮辱性：（1）\n",
    "- 非侮辱性：（0）\n",
    "\n",
    "此案例使用的函数说明：\n",
    "- `loadDataSet`：创建实验数据集\n",
    "- `createVocabList`：生成词汇表\n",
    "- `setOfWord2Vec`：生成词向量\n",
    "- `get_trainMat`：所有词条向量列表\n",
    "- `trainNB`：朴素贝叶斯分类器训练函数\n",
    "- `classifyNB`：朴素贝叶斯分类器分类函数\n",
    "- `testingNB`：朴素贝叶斯分类器测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"创建实验数据集\n",
    "    Returns\n",
    "    -------\n",
    "    dataSet :\n",
    "        切分好的样本词条\n",
    "    classVec :\n",
    "        类标签向量\n",
    "    \"\"\"\n",
    "    dataSet=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "            ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "            ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "            ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "            ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop','him'],\n",
    "            ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] # 切分好的词条\n",
    "\n",
    "    classVec = [0,1,0,1,0,1] #类标签向量：1 代表侮辱性词汇；2 代表非侮辱性词汇\n",
    "    return dataSet,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet, classVec = loadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"将切分的样本词条整理成词汇表（不重复）\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataSet : \n",
    "        切分好的样本词条\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    vocabList :\n",
    "        不重复词汇表\n",
    "    \"\"\"\n",
    "    vocabSet = set()\n",
    "    for doc in dataSet:\n",
    "        vocabSet = vocabSet | set(doc) #取并集\n",
    "        vocabList = list(vocabSet)\n",
    "    return vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList = createVocabList(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、获得训练集向量\n",
    "生成词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWordVec(vocabList, inputSet):\n",
    "    \"\"\"根据vocabList词汇表 将inputSet向量化 向量的每个元素为1或0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabList : \n",
    "        词汇表（不重复）\n",
    "    inputSet : \n",
    "        切分好的词条列表中的一条\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    returnVec :\n",
    "        文档向量 词集模型\n",
    "    \"\"\"\n",
    "    returnVec = [0] * len(vocabList) #init\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(f\" {word} is not in my vocabulary!\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有词条向量列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainMat(dataSet):\n",
    "    \"\"\"生成训练集向量列表\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataSet\n",
    "        切分好的样本词条\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    trainMat\n",
    "        所有词条向量组成的列表\n",
    "    \"\"\"        \n",
    "    trainMat = []\n",
    "    vocabList = createVocabList(dataSet)\n",
    "    for inputSet in dataSet:\n",
    "        returnVec = setOfWordVec(vocabList, inputSet)\n",
    "        trainMat.append(returnVec)\n",
    "    return trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = get_trainMat(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、朴素贝叶斯分类器训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMat, classVec):\n",
    "    \"\"\"朴素贝叶斯分类器训练函数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainMat\n",
    "        训练文档矩阵\n",
    "    classVec\n",
    "        训练类别标签向量\n",
    "\n",
    "    Return\n",
    "    ---------\n",
    "    p0v\n",
    "        非侮辱类的条件概率数组\n",
    "    p1v\n",
    "        侮辱类的条件概率数组\n",
    "    pAb\n",
    "        文档属于侮辱类的概率\n",
    "    \"\"\"    \n",
    "    n = len(trainMat)\n",
    "    m = len(trainMat[0])\n",
    "    pAb = sum(classVec)/n\n",
    "    p0Num = np.zeros(m)\n",
    "    p1Num = np.zeros(m)\n",
    "    p0Denom = 0\n",
    "    p1Denom = 0\n",
    "    for i in range(n):\n",
    "        if classVec[i] == 1:\n",
    "            p1Num += trainMat[i]\n",
    "            p1Denom += sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1v = p1Num/p1Denom\n",
    "    p0v = p0Num/p0Denom\n",
    "    return p0v,p1v,pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0v,p1v,pAb = trainNB(trainMat, classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'help', 'licks', 'how', 'so', 'cute', 'steak', 'quit', 'take', 'love', 'ate', 'mr', 'dog', 'not', 'maybe', 'my', 'posting', 'worthless', 'stop', 'stupid', 'buying', 'please', 'problems', 'garbage', 'is', 'park', 'him', 'food', 'has', 'flea', 'I', 'dalmation']\n"
     ]
    }
   ],
   "source": [
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05263158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05263158, 0.05263158, 0.        ,\n",
       "       0.        , 0.        , 0.10526316, 0.05263158, 0.05263158,\n",
       "       0.        , 0.05263158, 0.10526316, 0.05263158, 0.15789474,\n",
       "       0.05263158, 0.        , 0.        , 0.05263158, 0.        ,\n",
       "       0.05263158, 0.05263158, 0.05263158, 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.        , 0.        , 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "       0.125     , 0.        , 0.        , 0.04166667, 0.        ,\n",
       "       0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.        , 0.08333333, 0.        , 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、测试朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0v, p1v, pAb):\n",
    "    \"\"\"朴素贝叶斯分类器分类函数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec2classify\n",
    "        待分类的词条数组\n",
    "    p0v\n",
    "        非侮辱类的条件概率数组\n",
    "    p1v\n",
    "        侮辱类的条件概率数组\n",
    "    pAb\n",
    "        文档属于侮辱类的概率\n",
    "    Return\n",
    "    ----------\n",
    "    0\n",
    "        属于非侮辱类\n",
    "    1\n",
    "        属于侮辱类\n",
    "    \"\"\"    \n",
    "    p1 = reduce(lambda x,y: x*y, vec2classify * p1v) * pAb\n",
    "    p0 = reduce(lambda x,y: x*y, vec2classify * p0v) * (1 - pAb)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB(testVec):\n",
    "    dataSet, classVec = loadDataSet()\n",
    "    vocabList = createVocabList(dataSet)\n",
    "    trainMat = get_trainMat(dataSet)\n",
    "    p0v, p1v, pAb = trainNB(trainMat, classVec)\n",
    "    thisOne = setOfWordVec(vocabList, testVec)\n",
    "    if classifyNB(thisOne, p0v, p1v, pAb) == 1:\n",
    "        print(f\"{testVec} is a abusive word!\")\n",
    "    else:\n",
    "        print(f\"{testVec} is not a abusive word!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['love', 'my', 'dalmation'] is not a abusive word!\n",
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['stupid', 'garbage'] is not a abusive word!\n"
     ]
    }
   ],
   "source": [
    "testVec1 = ['love','my','dalmation']\n",
    "testingNB(testVec1)\n",
    "\n",
    "testVec2 = ['stupid','garbage']\n",
    "testingNB(testVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现无法分类，两个计算结果都是0\n",
    "### 6、朴素贝叶斯改进之拉普拉斯平滑\n",
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计\n",
    "算 $p(w0|1)p(w1|1)p(w2|1)$。如果其中有一个概率值为0，那么最后的成绩也为0。显然，这样是不合理\n",
    "的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普\n",
    "拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问\n",
    "题。\n",
    "\n",
    "另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。我们在计算乘积时，由于大部\n",
    "分因子都很小，所以程序会下溢或者得不到正确答案。为了解决这个问题，对乘积结果取自然对数。通\n",
    "过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。\n",
    "下图给出函数 $f(x)$ 和 $ln(f(x))$ 的曲线。\n",
    "\n",
    "![](\\images/改进.png)\n",
    "\n",
    "发现两条曲线在相同区间内同时增加和减少，并且在相同点取到极值，虽然取值不同，但不影响最终结果，代码修改如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMat, classvec):\n",
    "    n = len(trainMat)\n",
    "    m = len(trainMat[0])\n",
    "    pAb = sum(classVec)/n\n",
    "    p0Num = np.ones(m)\n",
    "    p1Num = np.ones(m)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(n):\n",
    "        if classVec[i] == 1:\n",
    "            p1Num += trainMat[i]\n",
    "            p1Denom += sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1v = np.log(p1Num/p1Denom)\n",
    "    p0v = np.log(p0Num/p0Denom)\n",
    "    return p0v,p1v,pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0v, p1v, pAb = trainNB(trainMat, classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0v, p1v, pAb):\n",
    "    p1 = sum(vec2classify * p1v) + np.log(pAb)\n",
    "    p0 = sum(vec2classify * p0v) + np.log(1.0 - pAb)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: -7.694848072384611\n",
      "p1: -9.826714493730215\n",
      "['love', 'my', 'dalmation'] is not a abusive word!\n",
      "p0: -7.20934025660291\n",
      "p1: -4.702750514326955\n",
      "['stupid', 'garbage'] is a abusive word!\n"
     ]
    }
   ],
   "source": [
    "testVec1 = ['love', 'my', 'dalmation']\n",
    "testingNB(testVec1)\n",
    "\n",
    "testVec2 = ['stupid', 'garbage']\n",
    "testingNB(testVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垃圾邮件过滤\n",
    "### 1、获取数据集\n",
    "所有邮件文本数据放在 email 文件夹中，ham 文件夹中为非垃圾邮件，spam 文件夹中为垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_dataSet():\n",
    "    \"\"\"创建实验数据集\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    dataSet\n",
    "        带标签的实验数据集（DF格式）\n",
    "    \"\"\"    \n",
    "    ham = []\n",
    "    for i in range(1, 26):\n",
    "        file_path = '../03-Navie_Byes/expDataSet/email/ham/%d.txt' %(i)\n",
    "        # print(file_path)\n",
    "        data = open(file_path, encoding='gbk', errors='ignore').read()\n",
    "        ham.append([data,'ham'])\n",
    "    df1 = pd.DataFrame(ham)\n",
    "    spam = []\n",
    "    for i in range (1, 26):\n",
    "        file_path = '../03-Navie_Byes/expDataSet/email/spam/%d.txt' %(i)\n",
    "        # print(file_path)\n",
    "        data = open(file_path, encoding='gbk', errors='ignore').read()\n",
    "        spam.append([data,'spam'])\n",
    "    df2 = pd.DataFrame(spam)\n",
    "    dataSet = pd.concat([df1,df2],ignore_index=True)\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi Peter,\\n\\nWith Jose out of town, do you wan...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yay to you both doing fine!\\n\\nI'm working on ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHat is going on there?\\nI talked to John on e...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yo.  I've been working on my running website. ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was a guy at the gas station who told me...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hello,\\n\\nSince you are an owner of at least o...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zach Hamm commented on your status.\\n\\nZach wr...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This e-mail was sent from a notification-only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi Peter,\\n\\nThese are the only good scenic on...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ryan Whybrew commented on your status.\\n\\nRyan...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arvind Thirumalai commented on your status.\\n\\...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Thanks Peter.\\n\\nI'll definitely check in on t...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jay Stepp commented on your status.\\n\\nJay wro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LinkedIn\\n\\nKerry Haloney requested to add you...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hi Peter,\\n \\nThe hotels are the ones that ren...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yeah I am ready.  I may not be here because Ja...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Benoit Mandelbrot 1924-2010\\n\\nBenoit Mandelbr...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hi Peter,\\n\\n    Sure thing.  Sounds good.  Le...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LinkedIn\\n\\nJulius O requested to add you as a...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I've thought about this and think it's possibl...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>we saw this on the way to the coast...thought ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hi Hommies,\\n\\nJust got a phone call from the ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\\nSciFinance now automatically generates GPU-e...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ok I will be there by 10:00 at the latest.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>That is cold.  Is there going to be a retireme...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>--- Codeine 15mg -- 30 for $203.70 -- VISA Onl...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hydrocodone/Vicodin ES/Brand Watson\\n\\nVicodin...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Percocet 10/625 mg withoutPrescription 30 tabs...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>--- Codeine 15mg -- 30 for $203.70 -- VISA Onl...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>OEM Adobe &amp; Microsoft softwares\\nFast order an...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Bargains Here! Buy Phentermin 37.5 mg (K-25)\\n...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bargains Here! Buy Phentermin 37.5 mg (K-25)\\n...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>OrderCializViagra Online &amp; Save 75-90%\\n\\n0nli...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Buy Ambiem (Zolpidem) 5mg/10mg @ $2.39/- pill\\...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>OrderCializViagra Online &amp; Save 75-90%\\n\\n0nli...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>BuyVIAGRA 25mg, 50mg, 100mg,\\nBrandViagra, Fem...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>A home based business opportunity is knocking ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Codeine (the most competitive price on NET!)\\n...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Get Up to 75% OFF at Online WatchesStore\\n\\nDi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Get Up to 75% OFF at Online WatchesStore\\n\\nDi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Percocet 10/625 mg withoutPrescription 30 tabs...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Get Up to 75% OFF at Online WatchesStore\\n\\nDi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>You Have Everything To Gain!\\n\\nIncredib1e gai...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Experience with BiggerPenis Today! Grow 3-inch...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0     1\n",
       "0   Hi Peter,\\n\\nWith Jose out of town, do you wan...   ham\n",
       "1   Yay to you both doing fine!\\n\\nI'm working on ...   ham\n",
       "2   WHat is going on there?\\nI talked to John on e...   ham\n",
       "3   Yo.  I've been working on my running website. ...   ham\n",
       "4   There was a guy at the gas station who told me...   ham\n",
       "5   Hello,\\n\\nSince you are an owner of at least o...   ham\n",
       "6   Zach Hamm commented on your status.\\n\\nZach wr...   ham\n",
       "7   This e-mail was sent from a notification-only ...   ham\n",
       "8   Hi Peter,\\n\\nThese are the only good scenic on...   ham\n",
       "9   Ryan Whybrew commented on your status.\\n\\nRyan...   ham\n",
       "10  Arvind Thirumalai commented on your status.\\n\\...   ham\n",
       "11  Thanks Peter.\\n\\nI'll definitely check in on t...   ham\n",
       "12  Jay Stepp commented on your status.\\n\\nJay wro...   ham\n",
       "13  LinkedIn\\n\\nKerry Haloney requested to add you...   ham\n",
       "14  Hi Peter,\\n \\nThe hotels are the ones that ren...   ham\n",
       "15  yeah I am ready.  I may not be here because Ja...   ham\n",
       "16  Benoit Mandelbrot 1924-2010\\n\\nBenoit Mandelbr...   ham\n",
       "17  Hi Peter,\\n\\n    Sure thing.  Sounds good.  Le...   ham\n",
       "18  LinkedIn\\n\\nJulius O requested to add you as a...   ham\n",
       "19  I've thought about this and think it's possibl...   ham\n",
       "20  we saw this on the way to the coast...thought ...   ham\n",
       "21  Hi Hommies,\\n\\nJust got a phone call from the ...   ham\n",
       "22  \\nSciFinance now automatically generates GPU-e...   ham\n",
       "23         Ok I will be there by 10:00 at the latest.   ham\n",
       "24  That is cold.  Is there going to be a retireme...   ham\n",
       "25  --- Codeine 15mg -- 30 for $203.70 -- VISA Onl...  spam\n",
       "26  Hydrocodone/Vicodin ES/Brand Watson\\n\\nVicodin...  spam\n",
       "27  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "28  Percocet 10/625 mg withoutPrescription 30 tabs...  spam\n",
       "29  --- Codeine 15mg -- 30 for $203.70 -- VISA Onl...  spam\n",
       "30  OEM Adobe & Microsoft softwares\\nFast order an...  spam\n",
       "31  Bargains Here! Buy Phentermin 37.5 mg (K-25)\\n...  spam\n",
       "32  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "33  Bargains Here! Buy Phentermin 37.5 mg (K-25)\\n...  spam\n",
       "34  OrderCializViagra Online & Save 75-90%\\n\\n0nli...  spam\n",
       "35  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "36  Buy Ambiem (Zolpidem) 5mg/10mg @ $2.39/- pill\\...  spam\n",
       "37  OrderCializViagra Online & Save 75-90%\\n\\n0nli...  spam\n",
       "38  BuyVIAGRA 25mg, 50mg, 100mg,\\nBrandViagra, Fem...  spam\n",
       "39  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "40  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "41  A home based business opportunity is knocking ...  spam\n",
       "42  Codeine (the most competitive price on NET!)\\n...  spam\n",
       "43  Get Up to 75% OFF at Online WatchesStore\\n\\nDi...  spam\n",
       "44  Get Up to 75% OFF at Online WatchesStore\\n\\nDi...  spam\n",
       "45  Percocet 10/625 mg withoutPrescription 30 tabs...  spam\n",
       "46  Get Up to 75% OFF at Online WatchesStore\\n\\nDi...  spam\n",
       "47  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "48  You Have Everything To Gain!\\n\\nIncredib1e gai...  spam\n",
       "49  Experience with BiggerPenis Today! Grow 3-inch...  spam"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = get_dataSet()\n",
    "dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、使用 SKlearn 对训练集进行特征值抽取\n",
    "使用 `TfidVectorizer`，对文本信息进行特征值抽取\n",
    "\n",
    "`TfidVectorizer` = `TfidTransformer` + `CountVectorizer`\n",
    "\n",
    "- `CountVectorizer`: 将文本文档转换为计数矩阵\n",
    "- `TfidTransformer`: 将计数矩阵转换为标准化的 TF 或 TF-IDF 矩阵\n",
    "\n",
    "- TF(term-frequency)：词频\n",
    "- IDF(inverse document frequency)：逆文档频率\n",
    "- TF-IDF：TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(dataSet[0])\n",
    "data_tf = tf.transform(dataSet[0])\n",
    "# data_tf = tf.fit_transform(dataSet[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、切分训练集和测试集\n",
    "以 $1：4$ 比例切分测试集和训练集，使用 `Sklearn` 的 `train_test_split` 函数切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35    spam\n",
       "0      ham\n",
       "24     ham\n",
       "20     ham\n",
       "18     ham\n",
       "31    spam\n",
       "8      ham\n",
       "29    spam\n",
       "22     ham\n",
       "38    spam\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data_tf, dataSet[1], test_size=0.2)\n",
    "Xtest.shape[0]\n",
    "ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、训练模型\n",
    "使用多项式分布朴素贝叶斯和伯努利分布朴素贝叶斯两种方法分别对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "# 多项式分布\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(Xtrain, ytrain)\n",
    "mnb.score(Xtest, ytest) # 查看准确率\n",
    "\n",
    "# 伯努利分布\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(Xtrain, ytrain)\n",
    "bnb.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnbs = []\n",
    "bnbs = []\n",
    "for i in range(10):\n",
    "    mnb = MultinomialNB() \n",
    "    mnb_s = cross_val_score(mnb,data_tf,dataSet[1],cv=10).mean()\n",
    "    mnbs.append(mnb_s)\n",
    "    \n",
    "    bnb = BernoulliNB()\n",
    "    bnb_s = cross_val_score(bnb,data_tf,dataSet[1],cv=10).mean()\n",
    "    bnbs.append(bnb_s)\n",
    "\n",
    "plt.plot(range(1,11), mnbs, label = \"多项式朴素贝叶斯\")\n",
    "plt.plot(range(1,11), bnbs, label = \"伯努利朴素贝叶斯\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法总结\n",
    "- 朴素贝叶斯优点：\n",
    "  - 简单快速，预测表现良好\n",
    "  - 直接使用概率预测，容易理解\n",
    "  - 若变量独立这个条件成立，相比其他分类方法，朴素贝叶斯分类器性能更优，且只需少量训练模型\n",
    "  - 相较于数值变量，朴素贝叶斯分类器在多个变量的情况下表现更好，若是数值变量，需要正态分布假设\n",
    "\n",
    "这些优点可以使得朴素贝叶斯分类器通常很适合作为分类的初始解。如果分类效果满足要求，那么万事\n",
    "大吉，你获得了一个非常快速且容易解释的分类器。但如果分类效果不够好，可以尝试更复杂的分类模型\n",
    "\n",
    "- 朴素贝叶斯缺点：\n",
    "  - 如果分类变量的类别（测数据集）没有在训练数据集总被观察到，那这个模型会分配一个0（零）概率给它，同时也会无法进行预测，这通常被称为“零频率”，为了解决这个问题，可以使用平滑技术，拉普拉斯估计是其中最基础的技术\n",
    "  - 朴素贝叶斯也被称为 *bad estimator*，所以它的概率输出 *predict_proba* 不应该被太认真对待\n",
    "  - 朴素贝叶斯的另一个限制是独立预测的假设。在现实生活中，这几乎是不可能的，各变量之间或多或少都会存在互相影响\n",
    "\n",
    "- 朴素贝叶斯的应用：\n",
    "  - **实时预测**：朴素贝叶斯简单快速\n",
    "  - **多类预测**：朴素贝叶斯以多类别预测功能闻名\n",
    "  - **文本分类/垃圾邮件过滤/情感分析**：相比较其他算法，朴素贝叶斯的应用主要集中在文本分类（变量类型多，且更独立），具有较高成功率。因此被广泛应用于垃圾邮件过滤（识别垃圾邮件）和情感分析（在社交媒体平台分辨积极情绪和消极情绪的用户）\n",
    "  - **推荐系统**：朴素贝叶斯分类器和协同过滤结合使用可以过滤出用户想看到的和不想看到的东西\n",
    "\n",
    "- 提升朴素贝叶斯分类器性能的 Tips：\n",
    "  - 如果连续特征不是正态分布的，应使用各种不同的方法将其转换成正态分布\n",
    "  - 如果测试数据集具有“零频率”的问题，应使用平滑技术“拉普拉斯估计”修正数据集\n",
    "  - 删除重复出现的高度相关的特性，可能会丢失频率信息，影响效果\n",
    "  - 朴素贝叶斯分类在参数调整上选择有限，建议把重点放在数据预处理和特征选择上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8baf67d759dc41d084056de23fe305760a7ffcb21bd78a2ec1cf32b9bc94016f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
